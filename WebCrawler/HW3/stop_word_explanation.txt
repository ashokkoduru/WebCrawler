I have used the tf-idf approach for identifying the stop words. The weightage formula which I have applied is 

weight(term) = tf(term) * idk(term)

tf(term) = frequency of the term/Total no of terms in the corpus
idf(term) = Log(Total no of docs/doc frequency of the term)

The cutoff value which I have applied is 0.5. Generally a high weight in tf-idf is obtained by a high term frequency in a particular document and low document frequency in the whole corpus since it is an inverse relationship. 

The ratio inside the logarithm will be always greater than 1. Because no of documents a term appears will be less than or equal to total number of documents. As a term appears in more documents the ratio will be closer to one. which means the log of that value will move closer to zero. So the lower the weightage more frequent is the term

If the value is much closer to zero the retrieval model does not give proper documents since most of the documents(which makes the value inside the log closer to 1) contains that term and the querying will not be efficient.
  
of
the
and
a
in
to
is
for
by
also
as
with
on
an
that
from
it
which
see
are
at
or
be
this
has
other
more
its
was
have
such
not
links
can
external
their
one
been
these
all
into
but
than
some
new
used
may
use
most
they
many
there
world
through
between 
first
including
about
were
when
over
only